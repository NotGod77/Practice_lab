{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7Rata5DgKCw"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "We use these regularised Linear Regression models to tackle overfitting, basically\n",
        "we just add an extra penalty for the coeffficients sum or squared sum, which reflects in the total cost function\n",
        "and when gradient descent minimises the cost function, it also adjusts the coefficients, and ultimately converges\n",
        "to form an equation with non important feature coefficients as zero or close to it.\n",
        "\n",
        "\n",
        "L1 uses the sum of absolute values of coefficients in the penalty term\n",
        "L2 uses the sum of squares of coefficients in the penalty term\n",
        "another variable lambda is multiplied to the penalty term, which can be adjusted to tune the overall penalty strength.\n",
        "'''\n",
        "\n",
        "'''\n",
        "Finally, the equation we get, is a regularised fit to the training data, not very complex, and also not very simple,\n",
        "So our model generalises better\n",
        "'''\n",
        "\n",
        "'''\n",
        "We can use Lasso and Ridge for L1 and L2 respectively\n",
        "\n",
        "imports-\n",
        "\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "'''"
      ]
    }
  ]
}